# Plan Update: Reach Indexing Pipeline Embeddings Step (Mermaid `G -> H`)

## Context Snapshot (Current State)
- `notesrag index` currently does: input read (`--source`/`--stdin`) -> normalize/validate text -> produce deterministic `document_id` -> print success.
- No chunking module exists yet (`G` missing).
- No embedding generation exists yet (`H` missing).
- Dependencies already include `ollama` and `better-sqlite3`.
- Vendor docs now include Ollama embedding usage (`ollama.embed({ model, input })`) and note vectors are unit-length.
- Planning assumption for this phase: Ollama server is already running in the local environment.

## Goal
Move the codebase from current ingest-only flow to a working indexing pipeline up through:
1. `G`: chunking layer with deterministic chunk IDs and overlap support.
2. `H`: embedding generation per chunk via local Ollama.

This plan intentionally stops before full persistence/index wiring (`I+` in README mermaid), except for temporary in-memory flow needed to validate `G -> H`.

## Scope
- In scope:
  - Define chunking config + deterministic chunk output shape.
  - Integrate chunking into `notesrag index` execution path.
  - Add embedding client wrapper around Ollama.
  - Generate embeddings for each chunk (single and batched paths).
  - Add failure handling for common Ollama errors (server down/unreachable, model missing, empty response).
  - Add tests for chunking and embedding orchestration with mocked Ollama.
  - Document runtime assumption + troubleshooting (`ollama serve` when unreachable, model pull when missing).
- Out of scope in this update:
  - Persisting chunks/embeddings to SQLite tables.
  - Vector similarity search and retrieval flow.
  - Full query pipeline.

## Resolved Decisions
1. Chunking strategy: fixed-size sliding window first (robust across small text, long lines, and bulleted content).
2. Embedding model: `nomic-embed-text` only for this phase.
3. Embedding failure behavior: fail hard by default; no partial-success fallback mode in normal operation.

## Implementation Plan
1. Add chunking module (`src/chunk.js`)
- Implement deterministic chunking over normalized text.
- Support configurable `chunkSize` and `overlap` with safe defaults.
- Produce chunk records:
  - `chunkId` (deterministic, doc-scoped)
  - `documentId`
  - `index` (0-based)
  - `text`
  - optional offsets (`startChar`, `endChar`) for traceability
- Guardrails:
  - reject invalid config (`overlap >= chunkSize`, non-positive sizes)
  - skip empty chunks after trim

2. Add embedding adapter (`src/embed.js`)
- Wrap `ollama` client usage in one function:
  - `embedChunks({ model, chunks, host? })`
- Use batch embedding call for chunk texts (`input: string[]`) for efficiency.
- Validate response shape:
  - number of vectors equals number of chunks
  - each vector is non-empty numeric array
- Return merged structure:
  - `{ chunkId, text, embedding, dimensions }[]`

3. Extend CLI args for indexing embedding phase
- Update `src/parseArgs.js` to accept embedding controls:
  - `--embed-model <name>` (default `nomic-embed-text`)
- Keep current UX contract for existing flags unchanged.

4. Wire `G -> H` in CLI flow (`src/cli.js`)
- New index flow:
  - read payload -> ingest -> chunk -> embed -> emit success summary
- Keep existing success line stable; append optional diagnostics only if useful (for example `chunks=<n> dims=<d>`).
- Error mapping:
  - Ollama unreachable -> actionable CLI error stating embeddings require a running Ollama server and suggesting `ollama serve`
  - model unavailable -> actionable CLI error suggesting `ollama pull <model>`

5. Add tests
- `test/chunk.test.js`
  - deterministic output for fixed text/config
  - overlap behavior
  - boundary cases (tiny text, exact boundary, invalid config)
- `test/embed.test.js`
  - mocks Ollama client
  - validates batch mapping chunk->vector
  - validates error on malformed embedding payload
- Update `test/cli.integration.test.js`
  - add mocked embedding path success
  - add model/server failure messaging checks (including explicit unreachable-server message)

6. Docs update
- `README.md`
  - describe `G -> H` behavior now implemented
  - runtime assumption for normal indexing flow: Ollama server is already running
  - troubleshooting steps:
    - if server is not reachable, start it with `ollama serve`
    - if model is missing, run `ollama pull nomic-embed-text`
  - mention vectors are normalized according to vendor docs
- Optionally align `timeline.md` status markers with completed scope.

## Definition of Done
- `notesrag index` can produce chunk embeddings locally for valid text input.
- Chunk IDs are deterministic for same `(documentId, text, config)`.
- Batch embedding path is used and tested.
- CLI surfaces clear, actionable errors for unreachable server and missing model.
- Automated tests cover chunking logic and embedding orchestration.
- README reflects exact required setup and current stop point (`through H`, not `I`).
